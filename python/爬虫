爬虫
	１．基本的爬虫工作原理

	２．基本的http抓取工具，scrapy　功能非常强大的爬虫框架

	３．Bloom Filter: Bloom Filters by Example　判重过滤器

	４．分布式爬虫　
	维护一个所有集群机器能够有效分享的分布式队列　　　python-rq: https://github.com/nvie/

	５．rq和Scrapy的结合：darkrho/scrapy-redis · GitHub

	６．后续处理，网页析取(grangier/python-goose · GitHub)，存储(Mongodb)

	分布式爬虫
		利用多线程的原理让多个爬虫同时工作
		Scrapy + MongoDB + Redis  
		a）. Scrapy  基本的页面爬取
		b）. MongoDB  存储爬取的数据　分布式文档数据库
		c）.Redis 存储要爬取的网页队列

难点：
	与反爬虫的较量
	有效地判重（网页判重）
	有效地存储（数据库应该怎样安排）
	有效地信息抽取（比如怎么样抽取出网页上所有的地址抽取出来）
	及时更新（预测这个网页多久会更新一次）

基本的工作原理
	发送请求——获得页面——解析页面——下载内容——储存内容
	向服务器发送请求后，会得到返回的页面，通过解析页面之后，抽取想要的那部分信息，并存储在指定的文档或数据库中

	0. 爬虫的基本思路 
		a. 通过URL或者文件获取网页，
		b. 分析要爬取的目标内容所在的位置
		c. 用元素选择器快速提取目标内容
		d. 处理提取出来的目标内容 
		e. 存储处理好的目标内容 

流程
	解析页面

	信息抽取

	存储

	数据清洗
		pandas 包的基本用法来做数据的预处理，得到更干净的数据。
			以下知识点掌握就好：
			缺失值处理：对缺失数据行进行删除或填充
			重复值处理：重复值的判断与删除
			空格和异常值处理：清楚不必要的空格和极端、异常数据
			分组：数据划分、分别执行函数、数据重组
		正则表达式

	数据分析
		numpy 数据分析，类似matlab的模块
		pandas（基于numpy的数据分析模块）

Scrapy运行流程：

	引擎从调度器中取出一个链接(URL)用于接下来的抓取
	引擎把URL封装成一个请求(Request)传给下载器
	下载器把资源下载下来，并封装成应答包(Response)
	爬虫解析Response
	解析出实体（Item）,则交给实体管道进行进一步的处理
	解析出的是链接（URL）,则把URL交给调度器等待抓取

静态网站
	requests连接网站，处理http协议
	bs4　网页　－－－>　结构化数据
	re正则表达式

动态网站
	chrome开发者工具　查看网页加载时的交互
	requests表单提交抓取js

