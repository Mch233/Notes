# 线性回归 #

处理连续数据

1.写出假设函数

![](https://i.imgur.com/9GeK8tV.png)

2.均方误差

![](https://i.imgur.com/p2yxEDO.png)

3.计算导数

![](https://i.imgur.com/AIYeK9A.png)

4.计算梯度下降公式

![](https://i.imgur.com/kGIr6pS.png)

5.得出更新公式


----------


## 数据预处理 ##

	离散属性
		
		连续化（属性有序） 高、中、低 -> （1.0 ，0.5， 0.0）
		k维向量（属性无序）（0 0 1）

	特征缩放 p43

		保证这些特征都具有相近的尺度

## 假设函数 ##

![](https://i.imgur.com/0Fy0s0o.png)

## 参数估计 ##

	思想：假设函数 --> 均方差误差最小化（拟合数据）
	
![](https://i.imgur.com/18DH6FB.png)
		 
	 利用 梯度下降法 求解

![](https://i.imgur.com/Ld2pWbM.png)

求出代价函数的导数，即：

![](https://i.imgur.com/yOjoxkx.png)

带入梯度下降公式即可求出更新公式：

![](https://i.imgur.com/gnlw6ic.png)

## 过拟合 ##
	1.特征变量，选取、丢弃

	2.正则化 
	保留所有特征，减少参数的大小

![](https://i.imgur.com/oIoVMrL.png)

可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的
基础上令 θ 值减少了一个额外的值。

## 推广 ##

	1.单变量 --> 多变量
		参数向量化

	2.y --> lny

	3.多分类问题
		拆解为多个二分类问题