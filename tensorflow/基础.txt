代价函数（损失函数）
	平方误差

	求解代价函数最小值：梯度下降法
					  共轭梯度法（Conjugate Gradient）， 
					  局部优化法(Broydenfletcher goldfarb shann,BFGS)有限内存局部优化法(LBFGS)


归一化处理（特征缩放）
	面对多维特征问题的时候，要保证这些特征都具有相近的尺度，这将帮助梯
	度下降算法更快地收敛

收敛
	绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛

通常可以考虑尝试些学习率：𝛽 = 0.01 ， 0.03 ， 0.1 ， 0.3 ， 1 ， 3 ， 10

过拟合
	解决措施
		1.丢弃一些不能帮助我们正确预测的特征
		2.正则化。 保留所有的特征，在一定程度上减小这些参数的值
		选择𝜇的方法为：
		1.使用训练集训练出 12 个不同程度正则化的模型
		2.用 12 个模型分别对交叉验证集计算的出交叉验证误差
		3.选择得出交叉验证误差 最小的模型
		4.运用步骤 3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉


神经网络

梯度检验

随机初始化
	通常初始参数为正负𝜁之间的随机值  Theta1 = rand( , ) * (2*eps) – eps


评估机器学习算法的性能
	通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析
	1.对于训练集，当d较小时，模型拟合程度更低，误差较大；随着d的增长，拟合程
	度提高，误差减小
	2.对于交叉验证集，当d较小时，模型拟合程度低，误差较大；但是随着d的增长，误差呈现先减小后增大的趋势，转折点是模型开始过拟合训练数据集的时候
	3.训练集误差和交叉验证集误差近似时：偏差/欠拟合
	  交叉验证集误差远大于训练集误差时：方差/过拟合


	验证集模型的代价函数误差与 λ 的值绘制在一张图表上
	当 𝜇 较小时，训练集误差较小（过拟合）而交叉验证集误差较大
	随着 𝜇 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加

	学习曲线，训练集误差和交叉验证集误差作为训练集实例数量（𝑛）的函数绘制的图表


判断假设函数是否过拟合
	假设函数ℎ(x)进行画图，然后观察图形趋势

	1.训练集+验证集+测试集   必须分隔验证集和测试集
	训练集：训练出不同的参数 60%
	验证集：评估————选出一组表现最好的 20%
	测试集：得出数据 20%

	2.交叉验证   多用在小数据集，深度学习中不常用
	取出测试集数据，保留部分作为测试集，剩余分为多份，每一份都当作验证集，n-1份训练、1份验证


过拟合（方差大）
	获得更多的训练实例
	尝试减少特征的数量
	尝试增加正则化程度 λ
欠拟合（偏差大）
	尝试获得更多的特征
	尝试增加多项式特征
	尝试减少正则化程度 λ
