激活函数比较

	sigmoid
		优点：输出映射在（0，1）之间，用于输出层，求导容易

		缺点：
			1.容易产生梯度消失 
			这样,几乎就没有信号通过神经元传到权重再到数据了,因此这时梯度就对模型的更新没有任何贡献。
			为了防止饱和,必须对于权重矩阵的初始化特别留意


			2. sigmoid 函数不是关于原点中心对称的 
			导致后面网络层的输入也不是零中心的,进而影响梯度下降的运作

	tanh 

		容易产生梯度消失 ,但它的输出是零中心的

	ReLU 
		优点：能够快速收敛，有效的缓解了梯度消失的问题，而且它提供了神经网络的稀疏表达能力

		缺点：随着训练的进行，可能会出现神经元死亡，权重无法更新的情况。合理设置学习率,会降低这种情况的发生概率

	Leaky ReLU 
		为解决“ ReLU 死亡”问题的尝试
		其效果并不是很稳定

	Maxout 

		这样 Maxout 神经元就拥有 ReLU 单元的所有优点(线性和不饱和),而没有它的缺点(死亡的ReLU单元)。然而和 ReLU 对比,它每个神经元的参数数量增加了一倍,这就导致整体参数的数量激增

	softmax

		用于多分类神经网络输出


如何选择激活函数? 

	如果使用 ReLU,那么一定要小心设置 learning rate,而且要注意不要让你的网络出现很多 “dead” 神经元,如果这个问题不好解决,那么可以试试 Leaky ReLU、PReLU 或者 Maxout.

	最好不要用 sigmoid,可以试试 tanh,不过可以预期它的效果会比不上 ReLU 和 Maxout
