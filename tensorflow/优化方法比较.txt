优化方法比较
	Batch gradient descent
		梯度更新规则:
			采用整个训练集的数据来计算 cost function 对参数的梯度
		 
		缺点:计算起来非常慢，不能投入新数据实时更新模型

	SGD(Stochastic gradient descent)
		梯度更新规则:
			每次更新时对每个样本进行梯度更新，SGD 一次只进行一次更新，可以新增样本

		缺点:
			SGD 更新比较频繁，会造成 cost function 有严重的震荡
			选择合适的learning rate比较困难
			对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更新快一些对于不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了
			SGD容易收敛到局部最优，在某些情况下可能被困在鞍点

	Mini-batch gradient descent
		超参数设定值:
			n 一般取值在 50～256

		缺点:
			Mini-batch gradient descent 不能保证很好的收敛性，
			learning rate 如果选择的太小，收敛速度会很慢，如果太大，loss function 就会在极小值处不停地震荡甚至偏离
			有一种措施是先设定大一点的学习率，当两次迭代之间的变化低于某个阈值后，就减小 learning rate，不过这个阈值的设定需要提前写好，这样的话就不能够适应数据集的特点

	Momentum
		简介：
			SGD方法的一个缺点是，其更新方向完全依赖于当前的batch，因而其更新十分不稳定。解决这一问题的一个简单的做法便是引入momentum。
			momentum即动量，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力
		特点：
			momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛

	Nesterov Momentum

	Nesterov Momentum
		简介：
			首先，按照原来的更新方向更新一步，然后在该位置计算梯度值，然后用这个梯度值修正最终的更新方向。

	Adagrad
		特点：
			自适应地为各个参数分配不同学习率

		缺点：
			需要手工设置一个全局的初始学习率
			学习率是单调递减的，训练后期学习率非常小，使得训练提前结束
			η设置过大的话，会使regularizer过于敏感，对梯度的调节太大

	Adadelta
		对Adagrad进行了改进，为了解决 Adagrad 学习率急剧下降问题
		特点：
			训练初中期，加速效果不错，很快
			训练后期，反复在局部最小值附近抖动

	RMSprop
		简介：
			RMSprop 是 Geoff Hinton 提出的一种自适应学习率方法！
			为解决 Adagrad 学习率急剧下降问题

		超参数设定值:
			Hinton 建议设定 γ 为 0.9, 学习率 η 为 0.001。

		特点：
			RMSprop依然依赖于全局学习率
			适合处理非平稳目标
			对于RNN效果很好

	Adam
		特点：
			带有动量项的RMSprop，
			每一次迭代学习率都有个确定范围，使得参数比较平稳
			 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点
			对内存需求较小
			为不同的参数计算不同的自适应学习率
			也适用于大多非凸优化
			适用于大数据集和高维空间

		超参数设定值:
			建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8

如何选择？

	对于稀疏数据：尽量使用学习率可自适应的优化方法，不用手动调节，最好采用默认值（即 Adagrad, Adadelta, RMSprop, Adam
	RMSprop, Adadelta, Adam 在很多情况下的效果是相似的，整体来讲，Adam 是最好的选择）

	如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法

	SGD通常训练时间更长，容易陷入鞍点，但是在好的初始化和学习率调度方案的情况下，结果更可靠

